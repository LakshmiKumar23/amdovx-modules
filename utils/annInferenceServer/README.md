# annInferenceServer

## DESCRIPTION
Inference Server

## Command-line Usage
    % annInferenceServer [-p port] [-b default-batch-size] [-gpu <comma-separated-list-of-GPUs>] [-q <max-pending-batches>] [-w <server-work-folder>]

## Features

 - Works with annInferenceApp
 - TCP/IP client-server network communication protocol and message formats
 - Multi-GPU high-throughput live streaming batch scheduler
 - OpenVX NN graph compiler from pre-trained caffe models
